\section{Beschreibung der gemessenen Daten}
\subsection{Similarity}
Die Messgröße, die in unserem Experiment gemessen wurde, ist die Similarity $S$.
Diese beschreibt, welcher Anteil der gesendeten Informationen korrekt von Bob
rekonstruiert werden konnte. Dies lässt sich über folgende Relation ausdrücken:
\begin{equation}
S = \frac{N_{\mathrm{korr}}}{N_{\mathrm{ges}}}\,,
\end{equation}
wobei $N_{\mathrm{korr}}$ die Anzahl der Bits im Schlüssel von Bob ist,
die nach der Bereinigung durch ungleiche Basiswahl verbleiben und mit den von
Alice gesendeten Bits übereinstimmen. Die Similarity $S$ wird mit der Gesamtlänge
$N_{\mathrm{ges}}$ des Schlüssels normiert, so dass $S \in [0,1]$ ergibt.

Die Zahl der richtig rekonstruierten Bits hängt hierbei von der Wahl der
Umstände ab, wie zum Beispiel der Kalibration der optischen Linsen und Filter, oder
auch von der Laser-Puls-Frequenz. Um diese Abhängigkeiten zu untersuchen, haben
wir verschiedene Messreihen aufgenommen. Für jeweils eine eingestellte Frequenz
wurde der Dämpfungsexponent der Lichtquelle durch Hinzufügen oder Entfernen eines Filters
bekannter Stärke verändert und anschließend wurde die Similarity bestimmt. Die
zugehörigen Messdaten können in Anhang \ref{sec:messwerttabellen} eingesehen
werden.


\begin{figure}[htb]
 \centering
 \includegraphics[width=\columnwidth,keepaspectratio]{%
  ../tmp/similarity_vs_Hz}
 \caption{Similarity in Abhängigkeit von der Frequenz $f$}
 \label{fig:sim_frequenz}
\end{figure}

\begin{figure}[htb]
 \centering
 \includegraphics[width=\columnwidth,keepaspectratio]{%
  ../tmp/similarity_vs_alpha1}
 \caption{Similarity in Abhängigkeit vom Dämpfungsexponenten α bei Frequenzen%
 von $f\in [50;500]$ Hz}
 \label{fig:sim_alpha1}
\end{figure}

\begin{figure}[htb]
 \centering
 \includegraphics[width=\columnwidth,keepaspectratio]{%
  ../tmp/similarity_vs_alpha2}
 \caption{Similarity in Abhängigkeit vom Dämpfungsexponenten α bei Frequenzen%
 von $f\in [1;10]$ kHz}
 \label{fig:sim_alpha2}
\end{figure}

\subsection{Einfluss der Parameter auf die Similarity}

Zunächst wurde die Frequenzabhängigkeit der Similarity untersucht (s.
\fref{sim_frequenz}). Mit kleiner werdenden Frequenzen nimmt die Similarity
unabhängig von der
gewählten Dämpfung ab. Eine Begründung hierfür könnte der Einfluss des
Streulichtes sein. Als mögliche Quellen wäre das verbleibende Umgebungslicht 
im Labor denkbar, danaben aber auch Wärmestrahlung. Thermische Prozesse im
Detektor tragen ebenfalls ein Teil zum Fehler bei.
Wenn man davon ausgeht, dass die Anzahl der Streuphotonen pro
Zeitintervall konstant bleibt, so steigt der relative Fehler, wodurch die
Similarity verringert wird, wenn im gleichen Zeitfenster nur wenige Photonen des
Signals den
Detektor erreichen. Bei hohen Frequenzen dagegen macht der Anteil des
Streulichtes am Gesamtsignal im Detektor nur einen geringen Prozentsatz aus,
wodurch die Similarity größere Werte annimmt.

Es kann davon ausgegangen werden, dass ab einer optimalen Frequenz $f_0(α)$ die
Similarity wieder abnimmt, da die Trägheit/Schaltgeschwindigkeit der EOMs nicht
mehr vernachlässigbar ist oder die Zeitauflösung des Detektors nicht mehr
ausreichend ist. Weiterhin gelangen bei sehr hohen Frequenzen auch die Signallaufzeiten
der elektrischen Anschlüsse an Relevanz.

Auf der anderen Seite überträgt sich auch die Unsicherheit in der Kalibration
der verwendeten Polarisationszustände auch auf die Similarity, die ihrerseits
ebenso ein Ausdruck für die Unsicherheit der Übertragung ist. Sind die
EOMs nicht exakt orthogonal oder parallel ausgerichtet, so ist die
Wahrscheinlichkeit nicht verschwindend, dass ein Photon bei zueinander
orthogonalen/parallelen Zuständen dennoch gemessen/nicht gemessen wird. Dieser
Fehler führt direkt zur Reduzierung der Similarity.

Durch die Wechselwirkung mit Materie kann ein Photon auf dem Weg von Alice zu
Bob ebenfalls seine spezifische Polarisation verlieren. Daher sollte ein Aufheizen,
Einstauben, oder Verschmutzen der Apparatur möglichst ausgeschlossen werden, dies
lässt sich jedoch nur unter größerem Aufwand erreichen.

Weiterhin bewirkt die konkrete Wahl des Wertes des Dämpfungsexponenten bei
konstant gehaltener Frequenz verschiedene Similarity-Werte. In \fref{sim_alpha1}
und \fref{sim_alpha2} sind die Verläufe für verschiedene Frequenzen dargestellt.

Wie man erkennen kann, nimmt die Similarity unabhängig von der gewählten
Frequenz mit steigender Dämpfung ab. Eine Ausnahme bildet zum einen die mit
\SI{50}{Hz} aufgenommene Kurve und zum anderen die zu einem Exponenten von $α=6$
gehörenden Punkte in \fref{sim_alpha2}. Eine stärkere Dämpfung bedeutet weniger
Photonen, die pro Laserpuls den Detektor erreichen können. Somit wächst, wie
schon weiter oben beschrieben, der Einfluss von in der Apparatur vorhandenem
Streulicht, was zu geringeren Similarity-Werten führt.

Für geringere Dämpfungen können im Mittel mehr Photonen pro Puls die Filter
passieren. Dies führt zu einer Zunahme an Messwert-Anzahlen und somit zur
Verringerung von statistischen Unsicherheiten bzw. Schwankungen. Andererseits
kann es somit durch Verunreinigungen der optischen Elemente zu einer Zunahme
von Streulicht kommen, was zu einer Verringerung der Similarity führt. Da aber
die Similarity für kleiner werdende Exponenten generell größere Werte annimmt,
lässt sich vermuten, dass diese Streulichtzunahme klein ist im Vergleich zum
größeren Einfluss des Streulichts bei größeren Dämpfungsexponenten.

Die resultierenden Schlüssellängen wurden ebenfalls gemessen und notiert. Diese
sind in \ref{sec:keylength} aufgelistet. Man kann erkennen, dass die relativen 
Schlüssellängen für $f=\SI{5}{kHz}$ am größten werden und außerdem ein
Dämpfungsexponent von fünf in der Regel die längsten relativen Schlüssel erzeugt.
Dies ist für eine schnelle Schlüsselerzeugung und somit auch
Informationsübertragung wichtig.

In einer realen Anwendung würde man allerdings aus anderen Gründen nicht zu
kleinen Dämpfungen übergehen, da es hier bedingt durch die Wahl
eines Lasers als Lichtquelle wahrscheinlicher wird, dass mehr als ein Photon
gleichzeitig die Apparatur durchqueren, was dazu führt, dass ein potentieller
Spion größere Chancen hat, unentdeckt zu bleiben und somit unbemerkt partielle
Kenntnis über den Schlüssel erlangen kann.

Um die gemessenen Daten genauer untersuchen zu können, wird eine Theorie
benötigt, die den Einfluss der Veränderung der beiden betrachteten Parameter in
einer Formel beschreibt. Mit einer solchen Formel könnte man die gemessenen
Daten fitten und so einen quantitativen Vergleich zwischen Erwartung und
Experiment anstellen. Da uns keine solche Theorie bekannt ist und nur relativ
wenige Messpunkte vorliegen, konnte kein sinnvoller Fit erstellt werden. Daher
musste sich auf die Betrachtung von Tendenzen beschränkt werden.

Schließlich wurden die aus den Unsicherheiten der Parameter resultierenden 
Fehlerbalken in alle gezeigten Histogramme hinzugefügt. Hierfür wurde
angenommen, dass sich der Einfluss der Unsicherheit der Frequenzeinstellung
sowie des Dämpfungsexponenten auf ein Prozent Abweichung der Similarity
beläuft. Dies ist nicht als korrekter Wert anzunehmen, sondern als Abschätzung
der Größenordnung der Unsicherheiten von Experimentatorseite. Weitere
Untersuchungen zu den vorhandenen Fehlerquellen und deren Einfluss müssen in
weiterführenden Experimenten durchgeführt werden.

Auch systematische Unsicherheiten, wie sie durch die bereits erwähnte
Fehlkalibrierung der EOMs, systematische Abweichungen der Filterplättchen oder
auch die Reinheit der Polarisation des Lichts nach dem Passieren der
Polarisationsfilter verursacht werden könnten, können ohne weitere Messungen
nicht quantitativ betrachtet werden.